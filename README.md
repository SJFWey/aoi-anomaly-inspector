# AOI Anomaly Inspector

Industrial Automated Optical Inspection (AOI) pipeline supporting **PaDiM** and **PatchCore** models, outputting **OK/NG decisions** and **defect localization** results.

Developed on the [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) dataset, enables one-click inference on any image folder with overlay visualizations, defect masks, and structured JSON reports.

## Model Comparison

### Metrics Comparison (transistor)

| Metric | PaDiM | PatchCore |
|--------|-------|-----------|
| **Image AUROC** | 0.9108 | **0.9954** |
| **Pixel AUROC** | **0.9685** | 0.9607 |

- **Image Size**: 256×256
- **Device**: CPU
- **Backbone**: ResNet-18 (PaDiM default layers: `layer1, layer2, layer3`; PatchCore default: `layer2, layer3`)
- **Threshold Strategy**: Validation-tuned (F1 optimization)

> **Conclusion**: PatchCore performs better in image-level detection (Image AUROC: 99.54%); PaDiM has a slight advantage in pixel-level segmentation (Pixel AUROC: 96.85%).

Metrics and visuals above are from the sample artifacts in `comparison_samples/` (generated by `compare_models.py`); results will vary by run and dataset.

### Sample Visualizations

The figures below show anomaly heatmap comparisons (overlays) of the two models on the same images. Left column is PaDiM, right column is PatchCore.

#### Good Samples

<table>
<tr>
<th>PaDiM</th>
<th>PatchCore</th>
</tr>
<tr>
<td><img src="comparison_samples/padim/good_01_overlay.png" width="200"/></td>
<td><img src="comparison_samples/patchcore/good_01_overlay.png" width="200"/></td>
</tr>
<tr>
<td><img src="comparison_samples/padim/good_02_overlay.png" width="200"/></td>
<td><img src="comparison_samples/patchcore/good_02_overlay.png" width="200"/></td>
</tr>
</table>

#### Defect Samples

<table>
<tr>
<th>Defect Type</th>
<th>PaDiM</th>
<th>PatchCore</th>
</tr>
<tr>
<td>bent_lead (Mild)</td>
<td><img src="comparison_samples/padim/defect_mild_bent_lead_01_overlay.png" width="200"/></td>
<td><img src="comparison_samples/patchcore/defect_mild_bent_lead_01_overlay.png" width="200"/></td>
</tr>
<tr>
<td>cut_lead (Mild)</td>
<td><img src="comparison_samples/padim/defect_mild_cut_lead_02_overlay.png" width="200"/></td>
<td><img src="comparison_samples/patchcore/defect_mild_cut_lead_02_overlay.png" width="200"/></td>
</tr>
<tr>
<td>damaged_case (Medium)</td>
<td><img src="comparison_samples/padim/defect_medium_damaged_case_03_overlay.png" width="200"/></td>
<td><img src="comparison_samples/patchcore/defect_medium_damaged_case_03_overlay.png" width="200"/></td>
</tr>
<tr>
<td>misplaced (Medium)</td>
<td><img src="comparison_samples/padim/defect_medium_misplaced_04_overlay.png" width="200"/></td>
<td><img src="comparison_samples/patchcore/defect_medium_misplaced_04_overlay.png" width="200"/></td>
</tr>
<tr>
<td>bent_lead (Severe)</td>
<td><img src="comparison_samples/padim/defect_severe_bent_lead_05_overlay.png" width="200"/></td>
<td><img src="comparison_samples/patchcore/defect_severe_bent_lead_05_overlay.png" width="200"/></td>
</tr>
<tr>
<td>cut_lead (Severe)</td>
<td><img src="comparison_samples/padim/defect_severe_cut_lead_06_overlay.png" width="200"/></td>
<td><img src="comparison_samples/patchcore/defect_severe_cut_lead_06_overlay.png" width="200"/></td>
</tr>
</table>

---

## Quick Start

### Environment

Use `uv` to manage dependencies (see `pyproject.toml` / `uv.lock`):

```bash
uv sync
uv pip install -e .
```

If you prefer `pip`:

```bash
pip install -e .
```

Core training/evaluation/inference/export scripts import the `aoi` package, so an editable install is required for those commands.

## Dataset

Organized by default according to anomalib's MVTec AD directory structure, with `datasets/mvtech/` pointing to the dataset root directory in the repository.

Directory structure should look like:

```text
datasets/mvtech/
  transistor/
    train/good/*.png
    test/<defect>/*.png
    ground_truth/<defect>/*.png
```

Quick check (will generate `preview.png`):

```bash
python scripts/check_data.py
```

## Training

### PaDiM

```bash
python scripts/train.py --config configs/padim_mvtec.yaml --run_id <padim_run_id> --device cpu
```

### PatchCore

```bash
python scripts/train.py --config configs/patchcore_mvtec.yaml --run_id <patchcore_run_id> --device cpu
```

Common override parameters (applicable to both models):

```bash
python scripts/train.py --config <cfg.yaml> --category transistor --data_root datasets/mvtech --device cpu --run_id <run_id>
```

## Evaluation and Thresholds

Recalculate metrics and generate thresholds for a specific run directory:

```bash
# Calculate metrics.json (Image AUROC / Pixel AUROC)
python scripts/evaluate.py --run_dir runs/padim/transistor/<padim_run_id> --device cpu

# Tune thresholds on validation set (recommended)
python scripts/evaluate.py --run_dir runs/padim/transistor/<padim_run_id> --tune-on-validation --pixel-metric f1
```

### Threshold Tuning Options

| Option | Description |
|--------|-------------|
| `--tune-on-validation` | Tune thresholds using validation set (recommended) |
| `--pixel-metric f1` | Optimize pixel threshold for F1 score |
| `--pixel-metric iou` | Optimize pixel threshold for IoU |
| `--compute-thresholds` | Compute quantile-based thresholds from train data |

> **Recommendation**: Use `--tune-on-validation --pixel-metric f1` for best results.

## Inference (Generate overlay / mask / JSON)

```bash
python scripts/predict.py --run_dir runs/patchcore/transistor/<patchcore_run_id> --device cpu
```

Use `--pixel-threshold-mult` to adjust sensitivity (predict.py default: `0.7`; aoi_runner.py default: `1.0`).

Output to `<run_dir>/predictions/`:
- `preds.jsonl`: Structured prediction results for each image
- `masks/`: Binary defect masks (enabled by default; disable with `--no-save-masks`)
- `overlays/`: Heatmap overlay visualizations (enabled by default; disable with `--no-save-overlays`)

Overlays are generated from `original_image` when available, otherwise the model input tensor; values are normalized to `0..255` for visualization.

## AOI Runner (One-Click Inference on Any Folder)

```bash
python scripts/aoi_runner.py \
  --input_dir <your_images/> \
  --output_dir outputs/demo \
  --model_dir runs/patchcore/transistor/<patchcore_run_id> \
  --device cpu
```

Default `--pixel-threshold-mult` is `1.0`; see `python scripts/aoi_runner.py --help` for recommended values.

### Output Directory Structure

```text
outputs/demo/
  overlays/       # Heatmap overlay visualizations
  masks/          # Binary defect masks
  preds.jsonl     # Structured predictions for each image
  report.json     # Summary report
```

### preds.jsonl Field Description

```json
{
  "image_path": "path/to/image.png",
  "model": "patchcore",
  "category": "transistor",
  "image_size": [256, 256],
  "gt_label": 1,
  "pred_score": 25.67,
  "image_threshold": 12.02,
  "pixel_threshold": 10.75,
  "effective_pixel_threshold": 10.75,
  "label": "NG",
  "is_anomaly": true,
  "num_defects": 3,
  "total_defect_area": 1250,
  "defects": [
    {
      "component_id": 1,
      "area": 800,
      "bbox": [50, 60, 120, 150],
      "centroid": [85.5, 105.2],
      "max_anomaly_value": 25.67,
      "mean_anomaly_value": 18.34
    }
  ],
  "mask_path": "masks/000001_image.png",
  "overlay_path": "overlays/000001_image.png"
}
```

`mask_path` and `overlay_path` are omitted when the corresponding outputs are not saved. For AOI Runner on arbitrary images, `gt_label` will be `null`.

### report.json Field Description

```json
{
  "model": "patchcore",
  "category": "transistor",
  "run_id": "<patchcore_run_id>",
  "thresholds": {
    "image_threshold": 12.02,
    "pixel_threshold": 10.75,
    "quantile_image": 0.0,
    "quantile_pixel": 0.0
  },
  "num_images": 100,
  "num_ok": 40,
  "num_ng": 60,
  "ng_rate": 0.6,
  "ok_rate": 0.4,
  "score_stats": {
    "min": 1.23,
    "max": 45.67,
    "mean": 12.34
  },
  "defect_stats": {
    "total_defects": 120,
    "avg_defects_per_ng": 2.0,
    "max_defect_area": 5000,
    "avg_total_defect_area": 1500.0
  },
  "created_at": "2026-01-29T15:00:00+00:00"
}
```

If `preds.jsonl` is empty, counts are zero and rates are `0.0`; `defect_stats` is `null` and `score_stats` is omitted.

## Model Comparison Script

Generate comparison tables and sample visualizations:

```bash
python scripts/compare_models.py \
  --run_dir_a runs/padim/transistor/<padim_run_id> \
  --run_dir_b runs/patchcore/transistor/<patchcore_run_id> \
  --output_dir comparison_samples
```

Output:
- `comparison_table.md`: Comparison table in Markdown format
- `comparison_samples/padim/`: PaDiM sample images
- `comparison_samples/patchcore/`: PatchCore sample images
- `comparison_grids/`: Side-by-side overlay grids (when overlays exist)
- `comparison_report.json`: Complete comparison report
- `sample_visualizations.md`: Markdown snippet for sample visuals

Sample visualizations require `predictions/preds.jsonl` and overlay files; if missing, only `comparison_table.md` is guaranteed and sample visuals may be incomplete.

## Model Export and Consistency Check

### ONNX Export

Export trained models to ONNX format for easy deployment:

```bash
python scripts/export.py --run_dir runs/padim/transistor/<padim_run_id>
python scripts/export.py --run_dir runs/patchcore/transistor/<patchcore_run_id>
```

Exported artifacts are saved to `<run_dir>/export/model.onnx`.

### Consistency Check

Verify inference consistency between ONNX exported models and original PyTorch models:

```bash
python scripts/consistency_check.py --run_dir runs/padim/transistor/<padim_run_id>
python scripts/consistency_check.py --run_dir runs/patchcore/transistor/<patchcore_run_id>
```

Check items include:
- Mean Absolute Error (MAE) of image scores
- Pixel-level error of anomaly maps
- IoU consistency of binary masks

### Consistency Check Results

| Model | Score MAE | Map MAE | Mask IoU | Status |
|------|-----------|---------|----------|------|
| PaDiM | 1.37e-05 | 8.91e-06 | 1.0 | ✅ PASSED |
| PatchCore | 2.10e-06 | 2.17e-06 | 1.0 | ✅ PASSED |

> Both models' ONNX exports passed consistency checks, with errors within allowable limits (score_tolerance=0.001, map_tolerance=0.001, mask_iou_threshold=0.99).

## Artifact Structure

Each training run outputs to:

```text
runs/<model>/<category>/<run_id>/
  config.yaml           # Training config snapshot
  meta.json             # Metadata
  weights/model.ckpt    # Model weights
  preds_train.jsonl     # Training set inference scores
  preds_test.jsonl      # Test set inference scores
  metrics.json          # Evaluation metrics
  thresholds.json       # Thresholds (generated by evaluate.py --compute-thresholds/--tune-on-validation; required for predict/aoi_runner)
  predictions/          # predict.py output
    preds.jsonl
    masks/
    overlays/
  export/               # ONNX export
    model.onnx
    export_meta.json
    consistency.json
  logs/                 # CSV logs (Lightning CSVLogger)
```

## Design Decisions

### Threshold Strategy

- **Validation-tuned thresholds**: Use F1-score optimization on validation set to find optimal thresholds
- **Image threshold**: Optimized for image-level classification (OK/NG decision)
- **Pixel threshold**: Optimized for pixel-level segmentation (defect mask generation)

| Model | Image Threshold (F1) | Pixel Threshold (F1) |
|-------|---------------------|---------------------|
| PaDiM | 40.00 (0.85) | 22.88 (0.61) |
| PatchCore | 13.68 (0.96) | 11.39 (0.62) |

### Post-processing Pipeline

1. Binarize the raw anomaly map using `pixel_threshold`
2. Connected component analysis (8-connected), extract bbox / area / centroid
3. Optional: Filter small defects (`--min-defect-area`)

### Engineering Reproducibility

- Fixed random seed (seed=42)
- Training configuration snapshot saved to `config.yaml`
- Dependencies locked in `uv.lock`

## Notes

- Default `data.num_workers=0`: Multi-process DataLoader in `torch` might trigger `PermissionError` in the current environment; if your environment supports multi-processing, you can increase this in `configs/*.yaml`.
- `PatchCore`'s `coreset_sampling_ratio` is set to `0.01` by default to ensure it runs within a reasonable time on CPU; if using GPU or aiming for higher performance, try increasing it to `0.1`.

## Full Workflow Example

The following shows the complete workflow from training to deployment:

```bash
# 1. Setup environment
uv sync
uv pip install -e .

# 2. Check data
python scripts/check_data.py

# 3. Train models
python scripts/train.py --config configs/padim_mvtec.yaml --run_id <padim_run_id> --device cpu
python scripts/train.py --config configs/patchcore_mvtec.yaml --run_id <patchcore_run_id> --device cpu

# 4. Evaluate and tune thresholds (validation-based)
python scripts/evaluate.py --run_dir runs/padim/transistor/<padim_run_id> --tune-on-validation --pixel-metric f1
python scripts/evaluate.py --run_dir runs/patchcore/transistor/<patchcore_run_id> --tune-on-validation --pixel-metric f1

# 5. Generate inference visualizations
python scripts/predict.py --run_dir runs/padim/transistor/<padim_run_id> --device cpu
python scripts/predict.py --run_dir runs/patchcore/transistor/<patchcore_run_id> --device cpu

# 6. Export to ONNX
python scripts/export.py --run_dir runs/padim/transistor/<padim_run_id>
python scripts/export.py --run_dir runs/patchcore/transistor/<patchcore_run_id>

# 7. Consistency check
python scripts/consistency_check.py --run_dir runs/padim/transistor/<padim_run_id>
python scripts/consistency_check.py --run_dir runs/patchcore/transistor/<patchcore_run_id>

# 8. Compare models
python scripts/compare_models.py \
  --run_dir_a runs/padim/transistor/<padim_run_id> \
  --run_dir_b runs/patchcore/transistor/<patchcore_run_id> \
  --output_dir comparison_samples

# 9. AOI Runner inference on arbitrary images
python scripts/aoi_runner.py \
  --input_dir <your_images/> \
  --output_dir outputs/demo \
  --model_dir runs/patchcore/transistor/<patchcore_run_id> \
  --device cpu
```

## Data License

This project uses the [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) dataset for development and validation. This dataset is restricted to academic research purposes only. For commercial use, please contact MVTec for authorization.
